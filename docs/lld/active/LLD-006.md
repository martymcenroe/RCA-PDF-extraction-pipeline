# 10006 - Feature: Analytics-Ready CSV Output Format

## 1. Context & Goal
* **Issue:** #6
* **Objective:** Add a third output format (`core_analysis_analytics.csv`) optimized for pandas, R, and Excel pivot tables
* **Status:** Approved
* **Related Issues:** #3 (related), #4 (related)
* **Effort Estimate:** S/M (Small to Medium)

### Open Questions
*Questions that need clarification before or during implementation. Remove when resolved.*

- [ ] Should `--analytics` be default or opt-in? (Proposed: opt-in)
- [ ] Should we generate analytics CSV alongside or instead of human-readable CSV?

## 2. Proposed Changes

*This section is the **source of truth** for implementation. Describe exactly what will be built.*

### 2.1 Files Changed

| File | Change Type | Description |
|------|-------------|-------------|
| `src/analytics_formatter.py` | Add | Transform extraction output to analytics format |
| `src/csv_sanitizer.py` | Add | CSV injection prevention (shared with #3) |
| `src/core_analysis_minimal.py` | Modify | Add `--analytics` CLI flag |
| `tests/test_analytics_format.py` | Add | Unit tests for analytics transformations |
| `tests/test_csv_sanitizer.py` | Add | Unit tests for CSV injection |
| `tests/fixtures/mock_raw_rows.json` | Add | Static fixture for offline development |

### 2.2 Offline Development Strategy

**Mock Mode:** The analytics formatter can be developed and tested independently using static JSON fixtures.

```python
# tests/fixtures/mock_raw_rows.json - Raw extraction rows for offline dev
[
    {"sample_number": "6-1(f)", "depth_feet": "9580.5", "permeability_air_md": "+", "saturation_water_pct": "35.2", ...},
    {"sample_number": "6-2(F)", "depth_feet": "9581.0", "permeability_air_md": "0.042", "saturation_water_pct": "**", ...},
    {"sample_number": "6-3", "depth_feet": "9581.5", "permeability_air_md": "<0.0001", "saturation_water_pct": "42.1", ...}
]
```

**Development Workflow:**
1. Create fixture with representative markers (+, **, (f), (F), <0.0001)
2. Develop `to_analytics_format()` using fixture - no PDF extraction needed
3. Develop `sanitize_csv_string()` using fixture
4. Integration test with real extraction as final step

### 2.3 Dependencies

*New packages, APIs, or services required.*

```toml
# Runtime: No new dependencies - uses Python standard library

[project.optional-dependencies]
dev = [
    "pandas>=2.0.0",  # Required for test scenario 050 (pandas compatibility check)
]
```

### 2.4 Data Structures

```python
# Pseudocode - NOT implementation

# Resource safety limit
MAX_ROWS = 10000  # Prevent memory spikes on unexpectedly large datasets

# Analytics row schema
AnalyticsRow = TypedDict('AnalyticsRow', {
    'sample_number': str,           # Cleaned, sanitized
    'is_fracture': bool,            # Extracted from (F)/(f) suffix
    'fracture_type': str,           # 'F', 'f', or 'NA'
    'depth_feet': float | str,      # Numeric or 'NA'
    'permeability_air_md': float | str,
    'permeability_klink_md': float | str,
    'permeability_below_detection': bool,  # TRUE if + or <0.0001
    'porosity_ambient_pct': float | str,
    'porosity_ncs_pct': float | str,
    'saturation_water_pct': float | str,
    'saturation_oil_pct': float | str,
    'saturation_total_pct': float | str,
    'saturation_no_data': bool,     # TRUE if ** marker
    'grain_density_gcc': float | str,
})
```

### 2.5 Function Signatures

```python
# src/analytics_formatter.py
class TransformationError(Exception):
    """Raised when row transformation fails."""
    pass

def to_analytics_format(rows: list[dict], max_rows: int = MAX_ROWS) -> list[dict]:
    """Transform extraction output to analytics-ready format.

    Args:
        rows: List of extracted row dicts
        max_rows: Maximum rows to process (default 10000)

    Raises:
        TransformationError: If transformation fails, includes sample_number and field

    Returns:
        List of analytics-formatted rows
    """
    ...

def to_numeric_or_na(val: str, sample_number: str, field_name: str) -> float | str:
    """Convert to float or 'NA' for non-numeric values.

    Logs conversion failures with sample_number and field for debugging.
    """
    ...

def validate_output_path(output_path: Path, project_root: Path) -> bool:
    """Validate output path is within project directory (worktree scope)."""
    ...

# src/csv_sanitizer.py
def sanitize_csv_string(val: str) -> str:
    """Escape formula characters to prevent CSV injection."""
    ...
```

### 2.6 Logic Flow (Pseudocode)

```
1. Receive extracted rows from PDF
2. Validate output path is within project worktree
   - IF invalid: raise ValueError, exit code 1
3. IF len(rows) > MAX_ROWS (10000):
   - Log warning, truncate to MAX_ROWS
4. FOR each row:
   a. Extract fracture indicator from sample_number
      - "6-1(F)" -> sample="6-1", is_fracture=TRUE, fracture_type="F"
   b. Handle below-detection markers
      - "+" or "<0.0001" -> NA with permeability_below_detection=TRUE
   c. Handle no-saturation markers
      - "**" -> NA across saturation columns with saturation_no_data=TRUE
   d. Convert all numeric fields to float or NA
   e. Sanitize string fields for CSV injection
   f. IF conversion fails:
      - Log ERROR with sample_number and field_name
      - Raise TransformationError
5. Write analytics CSV with transformed rows
6. IF transformation fails at any point:
   - Log ERROR with context (sample_number, field)
   - Print high-visibility warning to stderr
   - Return exit code 1 (non-zero)
   - Still produce standard outputs (CSV, JSON)
```

### 2.7 Technical Approach

* **Module:** `src/analytics_formatter.py`
* **Pattern:** Transformation layer between extraction and output
* **Key Decisions:**
  - Use string `NA` (compatible with R natively, pandas via `na_values=['NA']`)
  - Below-detection -> NA with flag (not 0.0001, avoids false precision)
  - Fail Open - transformation errors skip analytics file but don't abort

## 3. Requirements

*What must be true when this is done. These become acceptance criteria.*

1. Running with `--analytics` produces `core_analysis_analytics.csv`
2. No empty cells exist in analytics output (all converted to `NA`)
3. Numeric columns parse as `float64` in pandas with `na_values=['NA']`
4. Sample numbers contain no parenthetical suffixes
5. `is_fracture` column correctly identifies fracture samples
6. `permeability_below_detection` is `TRUE` for rows with `+` or `<0.0001`
7. `saturation_no_data` is `TRUE` for rows with `**` marker
8. Original `core_analysis.csv` unchanged (human-readable format preserved)
9. String fields with leading `=`, `+`, `-`, `@` are prefixed with `'`

## 4. Alternatives Considered

| Option | Pros | Cons | Decision |
|--------|------|------|----------|
| Replace standard CSV | Simpler output | Loses human-readable format | **Rejected** |
| JSON analytics format | Structured | Harder for Excel users | **Rejected** |
| Additional CSV with flags | Both formats available | Slight complexity | **Selected** |

**Rationale:** Providing both formats serves different user needs - human review vs automated analysis.

## 5. Data & Fixtures

### 5.1 Data Sources

| Attribute | Value |
|-----------|-------|
| Source | Extracted data from W20552.pdf |
| Format | In-memory dict from extraction |
| Size | 138 rows |
| Refresh | Per extraction run |
| Copyright/License | Assignment material |

### 5.2 Data Pipeline

```
PDF ──extract──► Raw rows ──to_analytics_format──► Analytics CSV
                    │
                    └──► Standard CSV (unchanged)
```

### 5.3 Test Fixtures

| Fixture | Source | Notes |
|---------|--------|-------|
| Rows with `+` markers | Generated | Test below-detection handling |
| Rows with `**` markers | Generated | Test no-saturation handling |
| Rows with `(F)`/`(f)` | Generated | Test fracture extraction |
| Injection payloads | Generated | Test `=SUM`, `@mention`, etc. |

### 5.4 Deployment Pipeline

Local development only - no deployment pipeline needed.

## 6. Diagram

N/A - Simple transformation pipeline, adequately described in logic flow.

## 7. Security Considerations

| Concern | Mitigation | Status |
|---------|------------|--------|
| CSV injection | Prepend `'` to formula characters | TODO |
| No external access | All processing local | Addressed |
| Worktree scope | Validate output path within project directory | TODO |
| Memory safety | MAX_ROWS = 10000 limit | Addressed |

**Fail Mode:**
- **Transformation failure:** Exit code 1 with high-visibility warning to stderr
- **Standard outputs:** Still produced even if analytics fails (partial success)

**Worktree Scope Requirement:**
Output path MUST be validated to reside within the project worktree before writing:
```python
def validate_output_path(output_path: Path, project_root: Path) -> bool:
    """Ensure output path is within project directory."""
    try:
        output_path.resolve().relative_to(project_root.resolve())
        return True
    except ValueError:
        return False
```

### 7.1 Logging Strategy

| Event | Level | Message Format |
|-------|-------|----------------|
| Transformation start | INFO | `Transforming {n} rows to analytics format` |
| Row transformed | DEBUG | `Transformed row: sample_number={sample}` |
| Conversion failure | ERROR | `Failed to convert field '{field}' for sample '{sample}': {error}` |
| Analytics skipped | WARNING | `Analytics CSV not generated due to transformation errors` |
| Analytics written | INFO | `Wrote analytics CSV: {path} ({n} rows)` |

## 8. Performance Considerations

| Metric | Budget | Approach |
|--------|--------|----------|
| Memory | < 50MB | Transform row by row |
| Latency | < 2s | Single pass transformation |

**Bottlenecks:** None expected for 138-row dataset.

## 9. Risks & Mitigations

| Risk | Impact | Likelihood | Mitigation |
|------|--------|------------|------------|
| Unexpected symbol not handled | Med | Low | Log warning, preserve as string |
| Type conversion error | Med | Low | Catch exception, use NA |

## 10. Verification & Testing

### 10.1 Test Scenarios

| ID | Scenario | Type | Input | Expected Output | Pass Criteria |
|----|----------|------|-------|-----------------|---------------|
| 010 | Empty cell -> NA | Auto | "" | "NA" | No empty cells |
| 020 | Below detection flag | Auto | "+" | NA, flag=TRUE | Correct conversion |
| 030 | No saturation flag | Auto | "**" | NA x3, flag=TRUE | All sat cols NA |
| 040 | Fracture extraction | Auto | "6-1(F)" | sample="6-1", is_fracture=TRUE | Correct split |
| 050 | Pandas compatibility | Auto | analytics CSV | df.dtype == float64 | Proper parsing |
| 060 | CSV injection escaped | Auto | "=SUM(A1)" | "'=SUM(A1)" | Leading quote |

### 10.2 Test Commands

```bash
# Run all automated tests
pytest tests/test_analytics_format.py tests/test_csv_sanitizer.py -v

# Verify pandas compatibility
python -c "import pandas as pd; df = pd.read_csv('data/output/spec/core_analysis_analytics.csv', na_values=['NA']); print(df.dtypes)"
```

### 10.3 Manual Tests (Only If Unavoidable)

| ID | Scenario | Why Not Automated | Steps |
|----|----------|-------------------|-------|
| 070 | R compatibility | Requires R installation | `read.csv()` in R, verify NA handling |

## 11. Definition of Done

### Code
- [ ] Implementation complete and linted
- [ ] Code comments reference this LLD

### Tests
- [ ] All test scenarios pass
- [ ] Test coverage meets threshold

### Documentation
- [ ] LLD updated with any deviations
- [ ] Implementation Report completed
- [ ] Update README with `--analytics` flag

### Review
- [ ] Code review completed
- [ ] User approval before closing issue

---

## Appendix: Review Log

*Track all review feedback with timestamps and implementation status.*

### Gemini 3 Pro Review #1 (REVISE)

**Timestamp:** 2026-01-29
**Reviewer:** Gemini 3 Pro (Senior Software Architect & AI Governance Lead)
**Verdict:** REVISE

#### Comments

| ID | Comment | Implemented? |
|----|---------|--------------|
| G1.1 | "Unbounded Transformation Loop (BLOCKING): Add row limit or chunking" | YES - Added MAX_ROWS = 10000 in Section 2.4 |
| G1.2 | "Worktree Scope (BLOCKING): Validate output path within project directory" | YES - Added validate_output_path() in Section 2.5 and 7 |
| G1.3 | "Fail-Safe Ambiguity (BLOCKING): Define non-zero exit code on failure" | YES - Added exit code 1 in Section 2.6 and 7 |
| G1.4 | "Offline Development (CRITICAL): Define JSON fixture for raw rows" | YES - Added Section 2.2 with mock_raw_rows.json |
| G1.5 | "Logging Context: Include sample_number and field in error logs" | YES - Updated to_numeric_or_na() signature and Section 7.1 |
| G1.6 | "Test Integrity: Add pandas as dev-dependency" | YES - Added to Section 2.3 |
| G1.7 | "Consider configurable filename via CLI" | DEFERRED - Hardcode for now |

---

### Gemini 3 Pro Review #2 (APPROVED)

**Timestamp:** 2026-01-30
**Reviewer:** Gemini 3 Pro (Senior Software Architect & AI Governance Lead)
**Verdict:** APPROVED

#### Comments

| ID | Comment | Implemented? |
|----|---------|--------------|
| G2.1 | "Clarify 'Fail Open' vs 'Partial Success' terminology in Implementation Report" | NOTED - Documentation detail |
| G2.2 | "Move MAX_ROWS to module-level constant or config" | NOTED - Implementation detail |

### Review Summary

| Review | Date | Verdict | Key Issue |
|--------|------|---------|-----------|
| Gemini 3 Pro #1 | 2026-01-29 | REVISE | Unbounded loop, worktree scope, fail-safe |
| Gemini 3 Pro #2 | 2026-01-30 | APPROVED | No blocking issues |

**Final Status:** APPROVED
